\documentclass[a4paper, 11pt, twoside, landscape]{article}
\usepackage[T1]{fontenc}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{multicol,multirow}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{IEEEtrantools}
%\usepackage{draculatheme}

\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=.1cm,left=.1cm,right=.1cm,bottom=.1cm} }
		{\geometry{top=.1cm,left=.1cm,right=.1cm,bottom=.1cm} }
	}
\pagestyle{empty}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\begin{document}
\begin{multicols}{4}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\subsection{Rules for the Mean and Variance}
\begin{IEEEeqnarray*}{rCl}
E(cX ) &=& cE(X) \\
E(X+Y) &=& E(X)+E(Y) \\
Var(cX) &=& c^2Var(X) \\
Var(X \pm Y) &=& Var(X) + Var(Y) \\
&\pm & 2Cov(X,Y) \\
Var(X) &=& \mathbb{E}[(X-\mathbb{E}[X])^2]
\end{IEEEeqnarray*}

\section{Bayesian Linear Regression}
\begin{IEEEeqnarray*}{rCl}
p(w|X,y) &=& \mathcal{N}(w;\bar{\mu}, \bar{\Sigma}) \\
\bar{\mu} &=& (X^TX+\frac{\sigma_n^2}{\sigma_p^2}I)^{-1}X^Ty \\
\bar{\Sigma} &=& (\frac{1}{\sigma_n^2}X^TX+\frac{1}{\sigma_p^2}I)^{-1} \\
y^\ast &=& w^T x^\ast + \epsilon \\
p(y^\ast | X, y, x\ast) &=& \mathcal{N}(\bar{\mu}^T x^\ast, {x^\ast}^T \bar{\Sigma} x^\ast+ \sigma_n^2) 
\end{IEEEeqnarray*}

\subsection{MLE and MAP regression}
\begin{IEEEeqnarray*}{rCl}
w_{MLE} &=& (X^TX)^{-1}X^Ty \\
w_{MAP} &=& (I\frac{\sigma_n^2}{\sigma_p^2} + X^TX)^{-1}X^Ty
\end{IEEEeqnarray*}

\section{Gaussian Proccesess}
$A \in \mathbb{R}^{m \times n}$,  $f_A$ is a collection of R.V s.t. $f_A \sim \mathcal{N}(\mu_A, K_{AA})$.
\begin{equation*}
  K_{AA} = \begin{bmatrix}
    k(x_1, x_1) & \ldots & k(x_1, x_m) \\
    \vdots & \ddots & \vdots \\
    k(x_m, x_1) & \ldots & k_(x_m, x_m)
  \end{bmatrix}
\end{equation*}
\begin{equation*}
  \mu_{A} = \begin{bmatrix}
    \mu(x_1)  \\
    \vdots  \\
    \mu(x_m) 
  \end{bmatrix}
\end{equation*}

For more than one new point $k(x, x')$ is a matrix like $K_{AA}$.
\begin{IEEEeqnarray*}{rCl}
\mu'(x) &=& \mu(x) \\
&+& k_{x,A}(K_{AA}+\sigma_n^2I)^{-1}(y_A - \mu_A)\\
k'(x, x') &=& k(x, x') \\
&-& k_{x, A}(K_{AA}+\sigma_n^2I)^{-1}k_{x', A}^T \\
  k_{x, A} &=& 
 \begin{bmatrix}
    k(x_1, x)  \\
    \vdots  \\
    k(x_m, x)
  \end{bmatrix} 
\end{IEEEeqnarray*}

\subsubsection{Online GP's}
$K_{AA} = k(x_{t+1}, x_{t+1})$ then calculate the posterior for a new arbitrary data point $x*$.

\subsection{Maximize the marginal likelihood of the data} 
$K(\theta)$ is the Kernel matrix. 
\begin{IEEEeqnarray*}{lll}
&& \underset{\theta}{\operatorname{argmax}} \; \int p(y_{train} \mid f, x_{train}, \theta) p(f \mid \theta) df \\
&=& \underset{\theta}{\operatorname{argmax}} \; \int \mathcal{N}(f(x), \sigma_n^2) \mathcal{N}(0, K(\theta)) df \\
&=& \underset{\theta}{\operatorname{argmax}} \; \mathcal{N} (0, K(\theta) + I\sigma_n^2) \\
&=& \underset{\theta}{\operatorname{argmax}} \; p(y_{train} \mid x_{train}, \theta)
\end{IEEEeqnarray*}
\section{Variational Inference}
\subsection{KL divergence} 
Reverse KL div: $KL(q||p)$. Forward KL: $KL(p||q)$ (gives more conservative variance estimates).
$$
KL(q||p) = \int q(\theta) \log\frac{q(\theta)}{p(\theta)} d\theta
$$

\subsection{Minimizing KL divergence}
\begin{IEEEeqnarray*}{rCl}
	&& \underset{q \in Q}{\operatorname{argmin}} \; KL(q||p(\theta |  y)) \\
	&=& \underset{q \in Q}{\operatorname{argmax}} \; \mathbb{E}_{\theta \sim q(\theta)}[\log p(\theta, y)] + H(q) \\
	&=& \underset{q \in Q}{\operatorname{argmax}} \; \mathbb{E}_{\theta \sim q(\theta)}[\log p(y | \theta)] - KL(q||p(\theta))  
\end{IEEEeqnarray*}

\subsection{Gradient of the ELBO}
\begin{IEEEeqnarray*}{rCl}
&& \nabla_\lambda \mathbb{E}_{\theta \sim q_\lambda}[f(\theta)] \\
&=& \mathbb{E}_{\epsilon \sim \phi}[\nabla_\lambda f(g(\epsilon ; \lambda))] \\
&=& \nabla_{C, \mu} \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I)}[\log p(y | C\epsilon + \mu)] \\ 
&=& n \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I)} \\
&& \mathbb{E}_{i \sim \mathcal{U}(1, .., m)} [\nabla_{C, \mu} \log p(y_i | C\epsilon + \mu x_i)] \\
&=& \frac{n}{m} \sum_{j=i}^m \nabla_{C, \mu} \log p(y_i | C\epsilon + \mu x_i)
\end{IEEEeqnarray*}

\section{MCMC methods}
\subsection{Hoeffding’s inequality} 
Given $f$ is bounded between $[0, C]$:
\begin{IEEEeqnarray*}{rCl}
&& P(|\mathbb{E}_P[f(X)] - \frac{1}{N}\sum_{i=1}^N f(x_i)| > \epsilon) \leq \\
&& 2\exp^{\frac{-2N\epsilon^2}{C^2}}
\end{IEEEeqnarray*}
Error less than $\epsilon$ with probability $1 - \delta$:
$$
2\exp^{\frac{-2N\epsilon^2}{C^2}} \leq \delta 
$$

\subsection{MH-MCMC} 
DBE: $Q(x)P(x'|x) = Q(x')P(x|x')$. 
\begin{IEEEeqnarray*}{rCl}
&& R(X'|X = x) \\
&& X_{t+1} = x', P(X_{t+1} = x') = \alpha \\
&& \alpha = min \; \big\{1, \frac{Q(x')R(x|x')}{Q(x)R(x'|x)} \big\} \\
&& \text{o.t.w} \; X_{t+1} = x
\end{IEEEeqnarray*}

\subsection{Continuous RV}
\begin{IEEEeqnarray*}{rCl}
&& p(x) = \dfrac{1}{Z} e^{-f(x)} \\
&& \alpha = min \; \big\{1, \frac{R(x|x')}{R(x'|x)} e^{f(x)-f(x')} \big\}
\end{IEEEeqnarray*}

If $R(x'|x) = \mathcal{N}(x, \tau I)$ then $\alpha = min \; \big\{1, e^{f(x)-f(x')} \big\}$. Guaranteed efficient convergence for log-concave densities ($f$ convex).

\subsection{Improved Proposals}
Metropolis adjusted Langevin (gradient for proposals), Stochastic Gradient Langevin Dynamics, Hamiltonian Monte Carlo (momentum).

\section{Bayesian Neural Networks}
\subsection{MAP estimation with BNN's}
\begin{IEEEeqnarray*}{rCl}
	\hat{\theta} &=& \underset{\theta}{\operatorname{argmin}} \; -\log p(\theta) - \sum_{i=1}^n \log p(y_i | x_i, \theta) \\
	&=& \underset{\theta}{\operatorname{argmin}} \;\lambda ||\theta ||_2^2 \\
	&+& \frac{1}{2} \sum_{i=1}^n \big[ \frac{1}{\sigma(x_i, \theta)^2} || y_i - \mu(x_i, \theta) ||_2^2 \\
	&+& \log\sigma(x_i, \theta)^2 \big]
\end{IEEEeqnarray*}

\subsection{Variational Inference in BNN's}
\begin{IEEEeqnarray*}{rCl}
&& p(y^* \mid x^*, X, y) \\
&=& \int p(y^* \mid x^*, \theta) p(\theta \mid X,y) d\theta \\
&=& \mathbb{E}_{\theta \sim p(\theta \mid X,y)} \big[  p(y^* \mid x^*, \theta) \big] \\
&\approx & \mathbb{E}_{\theta \sim q_\lambda} \big[  p(y^* \mid x^*, \theta) \big] \\
&\approx & \frac{1}{m} \sum_{j=1}^m p(y^* \mid x^*, \theta^{(j)}) \\
& = & \frac{1}{m} \sum_{j=1}^m \mathcal{N}(\mu(x^*, \theta), \sigma^2(x^*, \theta))
\end{IEEEeqnarray*}

\subsection{Uncertainty for Gaussians}
\begin{IEEEeqnarray*}{rCl}
&& Var[y^\star | X, y, x^\star] = \underset{aleat}{\mathbb{E}[Var[y^\star | x^\star, \theta]]}  \\
&& + \underset{epis}{Var[\mathbb{E}[y^\star | x^\star, \theta]]} \\
& &\approx \frac{1}{m} \sum_{j=1}^m \sigma^2(x^\star, \theta^{(j)}) \\
&& + \frac{1}{m} \sum_{j=1}^m \big( \mu(x^\star, \theta^{(j)}) - \bar{\mu}(x^\star)\big)^2 
\end{IEEEeqnarray*}

\subsection{MC Dropout and Probabilistic Ensembles}
$$
p(y^* \mid x^*, X, y) \approx \frac{1}{m} \sum_{j=1}^m p(y^* \mid x^*, \theta^{(j)})
$$

\section{Active Learning}
Given $Y = X + \epsilon$ and $\epsilon \sim \mathcal{N}(0, \sigma_n^2I)$.
\begin{IEEEeqnarray*}{rCl}
I(Y;X) &=& H(Y) - H(Y|X) \\
&=& H(Y) - H(\epsilon) \\
&=& \frac{1}{2} \ln |I + \sigma_n^{-2}\Sigma |
\end{IEEEeqnarray*}

\subsubsection{Uncertainty Sampling}
$S$ is the optimal set of observations, $S_t$ the greedy set. Following the same regression scheme as before. 
\begin{IEEEeqnarray*}{rCl}
&& I(f(x_T), y_T) \geq \bigg(1 - \frac{1}{e} \bigg) \underset{|S| \leq T}{\operatorname{max}} \; I(f(x_S), y_S) \\
&& x_{t+1} = \underset{x}{\operatorname{argmax}} \; \mathbb{I}(f; y_x | y_{S_t}) \\ 
&& = \underset{x}{\operatorname{argmax}} \;\frac{1}{2} \log \big(1 + \frac{\sigma_t^2(x)}{\sigma_n^2} \big) 
\end{IEEEeqnarray*}

\subsection{Active Learning for Classification}
Uncertainty sampling: $x_{t+1} = \operatorname{argmax}_x \; H(Y|x, X_t, Y_t)$. Better to use approximate inference to estimate MI:
\begin{IEEEeqnarray*}{rCl}
x_{t+1} &=& \underset{x \in D}{\operatorname{argmax}} \; \mathbb{I}(\theta; y_{t+1} | Y_t, X_t, x_{t+1}) \\
&=& H(y_{t+1} | Y_t, X_t, x_{t+1}) \\
&-& \mathbb{E}_{\theta \sim p(|X_t, Y_t)}[H(y_{t+1} | \cdot, \theta)] \\
&\approx & H(y_{t+1} | Y_t, X_t, x_{t+1}) \\
&-& \frac{1}{m} \sum_{j=1}^m H(y_{t+1} | \cdot, \theta^{(j)})
\end{IEEEeqnarray*}

\section{Bayesian Optimization}
\subsubsection{Cumulative Regret}
$$
\frac{1}{T} \sum_{t=1}^T [f(x^*) - f(x_t)] \to 0
$$

\subsubsection{Upper confidence sampling}
Convergence of the cumulative regret as a function of $\gamma_T = \underset{|S| \leq T}{\operatorname{max}} \; I(f;y_S)$

\subsection{Thomson Sampling}
Sample $\tilde{f} \sim \mathcal{P}(f | X_t, Y_t)$, and then $x_{t+1} \in \underset{x \in D}{\operatorname{argmax}} \; \tilde{f}(x)$.

\section{Markov Decision Processes}
\subsection{Expected Value of a Policy}
For a deterministic reward, some $\pi$ and state $x$:

\begin{IEEEeqnarray*}{rCl}
&& J(\pi | X_0 = x) = V^{\pi}(x) \\
&=& \mathbb{E}[\sum_{t=0}^\infty \gamma^t r(X_t, \pi(X_t)) | X_0 = x] \\
&=& r(x, \pi(x)) + \gamma \sum_{x'} P(x'|x, \pi(x))  V^{\pi}(x') \\
&& V^{\pi} = r^{\pi} + \gamma T^{\pi}V^{\pi} \\
&& V^{\pi} = (I - \gamma T^{\pi})^{-1}r^{\pi} 
\end{IEEEeqnarray*}

\subsubsection{Fixed Point Iteration} 
Loop $T$ times s.t. $V^{\pi}_t = r^{\pi} + \gamma T^{\pi}V^{\pi}_{t-1}$. Computational advantages for sparse solutions. 

\subsection{Policy Iteration}
Init. arbitrary (e.g., random) policy $\pi$. Compute $V^{\pi}$. Compute greedy policy $\pi_V(x) = \underset{a}{\operatorname{argmax}} \; r(x, \pi(x)) + \gamma \sum_{x'} P(x'|x, \pi(x))  V(x')$  w.r.t. the previously computed $V^{\pi}$. Set $\pi \leftarrow \pi_V$.   

\subsection{Value Iteration}
Init $V_0(x) = \underset{a}{\operatorname{max}} \; r(x, a)$. For $t = 1$ to $\infty$: For each $x, a$, $Q_t(x, a) = r(x, a) + \gamma \sum_{x'} P(x'|x, \pi(x))  V_{t-1}(x')$. For each  $x$, $V_t(x) = \underset{a}{\operatorname{max}} \; Q_t(x, a)$ Break if $\underset{x}{\operatorname{max}} \; |V_t(x) -V_{t-1}(x)| \le \epsilon$, otw repeat.
\section{Reinforcement Learning}
\subsection{Model Based RL}
\subsubsection{MLE}
\begin{IEEEeqnarray*}{rCl}
\hat{P}(X_{t+1} | X_t, A) &=& \frac{Count(X_{t+1}, X_t, A)}{Count(X_{t+1}, A)} \\
\hat{r} &=& \frac{1}{N_{x, a}} \sum_{t} R_t
\end{IEEEeqnarray*}

\subsubsection{Rmax Algorithm}
Add fairy tale state $x^{\star}$. Init $r(x, a) = Rmax$, $P(x^{\star} |x,a)=1$. Init $\pi |r,P$. Loop: Deploy $\pi$. If observed “enough” $P$ / $r$, recompute $\pi$.

\subsection{Model Free RL}
\subsubsection{TD-Learning}
Guarantees convergence conditional on $\alpha_t$.
$$
\hat{V}^{\pi}(x) = (1 - \alpha_t)\hat{V}^{\pi}(x) + \alpha_t(r + \gamma \hat{V}^{\pi}(x'))
$$

\subsubsection{SGD on the squared loss}
Old value estimates are labels/targets ($r + \gamma V(x'; \theta_{old}) = y$). Same insight applies for the $Q(x, a)$.
$$
l_2(\theta ; x, x', r) = \frac{1}{2}(V(x,\theta) - r - \gamma V(x'; \theta_{old}))^2
$$

\subsubsection{Q-learning}
Optimistic initialization = guaranteed convergence. General convergence if $\forall \; (a, x)$ are visited $\infty$ many times. Otw trade off with epsilon greedy strategy.
\begin{IEEEeqnarray*}{rCl}
&& Q^{\star}(x, a) = r(x,a) + \gamma \sum_{x'} P(x' | x, a) V^{\star}(x') \\
&& V^{\star}(x) = \underset{a}{\operatorname{max}} \; Q^{\star}(x, a) \\
&& Q^{\star}(x, a) \leftarrow (1 - \alpha_t)Q^{\star}(x, a) \\
&&+ \alpha_t(r + \gamma \underset{a'}{\operatorname{max}} \; Q^{\star}(x', a'))
\end{IEEEeqnarray*}
Unfeasible for continues state spaces because of memory requirement $\forall \; (a, x)$. 

\subsubsection{Approximating value functions}
$\phi(x, a)$ is a set of hand designed features. 
\begin{IEEEeqnarray*}{rCl}
&& \hat{Q}(x, a; \theta) = \theta^T \phi(x,a)\\
&& l_2(\theta ; x, a, x', r) = \frac{1}{2}(Q(x, a, \theta) - r \\
&& - \gamma \underset{a'}{\operatorname{max}} \; Q(x', a'; \theta_{old}))^2 \\
\delta &=& Q(x, a, \theta) - r - \gamma \underset{a'}{\operatorname{max}} \; Q(x', a'; \theta_{old}) \\
&& \theta \leftarrow \theta - \alpha_t \delta \nabla_{\theta} Q(x,a;\theta) \\
&& \theta \leftarrow \theta - \alpha_t \delta \phi(x, a) \\
&& L(\theta) = \sum_{(x, a, r, x') \in D} l_2(\theta ; x, a, x', r)
\end{IEEEeqnarray*}

\subsection{Policy search methods}
\begin{IEEEeqnarray*}{rCl}
\pi(x) &=& \pi(x,\theta) \\
r(\tau^{(i)}) &=& \sum_{t=0}^T \gamma^t r_t^{(i)} \\
J(\theta) &\approx & \frac{1}{m} \sum_{i=1}^m r(\tau^{(i)}) \\
\theta^{\star} &=& \underset{\theta}{\operatorname{argmax}} \; J(\theta)\\ 
\nabla_{\theta} J(\theta) &=& \nabla \mathbb{E}_{\tau \sim \pi_{\theta}}r(\tau) \\
&=& \mathbb{E}_{\tau \sim \pi_{\theta}}[r(\tau)\nabla\log\pi_{\theta}(\tau)]
\end{IEEEeqnarray*}

\subsection{REINFORCE Algorithm}
\begin{IEEEeqnarray*}{rCl}
&& \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_{t=0}^T r(\tau)\nabla\log\pi_{\theta}(a_t|x_t;\theta)] \\
&& \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_{t=0}^T (r(\tau) - b(\tau_{0:t-1})) \\
&& \nabla \log \pi_{\theta}(a_t|x_t;\theta)] \\
&& b(\tau_{0:t-1}) = \sum_{t'=0}^{t-1}\gamma^{t'}r_{t'} \\
&& \nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_{t=0}^T \gamma^t G_t \nabla \log \pi_{\theta}(a_t|x_t;\theta)] \\
&& G_t = \sum_{t'=t}^T \gamma^{t'-t} r_{t'} 
\end{IEEEeqnarray*}
Initialize policy weights $\pi(a|x;\theta)$. Repeat: Generate an episode. For every $t$ get $G_t$. Update $\theta \leftarrow \theta + \eta \gamma^t G_t \nabla_{\theta} \log \pi(A_t|X_t;\theta)$

\subsection{Policy Gradient Theorem}
$$
\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}[Q(x, a) \nabla\log\pi_{\theta}(a | x ; \theta)]
$$

\subsubsection{Online Actor Critic}
\begin{IEEEeqnarray*}{rCl}
\theta_{\pi} &\leftarrow & \theta_{\pi} + \eta_t Q(x, a; \theta_Q) \nabla \log \pi (a|x; \theta_{\pi}) \\
\theta_{Q} &\leftarrow & \theta_{Q} - \eta_t( Q(x, a; \theta_Q) - r \\
 &-& \gamma Q (x', \pi(x', \theta_{\pi}); \theta_{Q})) \nabla Q (a|x; \theta_{\pi})
\end{IEEEeqnarray*}


\end{multicols}
\end{document}
